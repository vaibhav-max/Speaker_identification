{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make the csv of the unique speakerid,district,state, no. of file and duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import wave\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to extract speaker ID, district, and state from filename\n",
    "def extract_info(filename):\n",
    "    parts = filename.split('_')\n",
    "    speaker_id = parts[2]\n",
    "    district = parts[1]\n",
    "    state = parts[0]\n",
    "    return speaker_id, district, state\n",
    "\n",
    "# Function to calculate duration of a .wav file\n",
    "def get_wav_duration(file_path):\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        frames = wav_file.getnframes()\n",
    "        rate = wav_file.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "        return duration\n",
    "\n",
    "# Path to the folder containing the .wav files\n",
    "folder_path = '/home/vaibh/Vaani/Speaker_ID/Dataset/megdap'\n",
    "\n",
    "# Dictionary to store speaker IDs, districts, states, durations, and their corresponding counts\n",
    "speaker_info = defaultdict(lambda: {'district': '', 'state': '', 'duration': 0, 'count': 0})\n",
    "\n",
    "# Iterate through .wav files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        speaker_id, district, state = extract_info(filename)\n",
    "        audio_path = os.path.join(folder_path, filename)\n",
    "        duration = get_wav_duration(audio_path)\n",
    "        speaker_info[speaker_id]['district'] = district\n",
    "        speaker_info[speaker_id]['state'] = state\n",
    "        speaker_info[speaker_id]['duration'] += duration\n",
    "        speaker_info[speaker_id]['count'] += 1\n",
    "\n",
    "# Write unique speaker IDs, districts, states, durations, and their counts to a CSV file\n",
    "output_csv_path = 'speaker_info_with_duration.csv'\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Speaker ID', 'District', 'State', 'Total Duration (s)', 'Number of Audio Files'])\n",
    "    for speaker_id, info in speaker_info.items():\n",
    "        writer.writerow([speaker_id, info['district'], info['state'], info['duration'], info['count']])\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  add the gender info in the above csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully with gender information added.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file containing speaker information (without gender)\n",
    "input_csv_path = '/home/vaibh/Vaani/Speaker_ID/speaker_info_with_duration.csv'\n",
    "\n",
    "# Path to the new CSV file to be created with gender information added\n",
    "output_csv_path = 'speaker_info_with_gender.csv'\n",
    "\n",
    "# Path to the CSV file containing speaker ID and gender information\n",
    "gender_csv_path = '/home/vaibh/Vaani/Speaker_ID/genderMeta_megdap_13Dec.csv'\n",
    "\n",
    "# Read speaker ID and gender information from existing CSV file\n",
    "gender_info = {}\n",
    "with open(gender_csv_path, 'r') as gender_file:\n",
    "    reader = csv.reader(gender_file)\n",
    "    for row in reader:\n",
    "        speaker_id, gender = row\n",
    "        #print(type(speaker_id), type(gender))\n",
    "        #gender_info[speaker_id] = gender\n",
    "        gender_info[speaker_id.strip()] = gender\n",
    "\n",
    "\n",
    "# speaker_id = ' 80475 '\n",
    "# gender = gender_info.get(speaker_id, '')  # Get gender info from the dictionary\n",
    "# print(gender)\n",
    "\n",
    "\n",
    "# Open input and output CSV files\n",
    "with open(input_csv_path, 'r') as input_file, open(output_csv_path, 'w', newline='') as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "\n",
    "    # Read header from input and add 'Gender' column to header\n",
    "    header = next(reader)\n",
    "    header.append('Gender')\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Iterate through rows in input CSV file\n",
    "    for row in reader:\n",
    "        # Get speaker ID from the row\n",
    "        speaker_id = row[0]\n",
    "        #print(type(speaker_id))\n",
    "        # Get gender information from the gender_info dictionary\n",
    "        gender = gender_info.get(speaker_id, '')  # Get gender info from the dictionary\n",
    "        #print(gender)\n",
    "        # Add gender information to the row\n",
    "        row.append(gender)\n",
    "        # Write the row to the output CSV file\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV file created successfully with gender information added.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  made the csv which contains all the information District Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data written to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to the existing CSV file\n",
    "input_csv_path = '/home/vaibh/Vaani/Speaker_ID/speaker_info_with_gender.csv'\n",
    "\n",
    "# Path to the new CSV file to be created\n",
    "output_csv_path = 'aggregated_data_district_wise.csv'\n",
    "\n",
    "# Dictionary to store aggregated data for each district and state\n",
    "aggregated_data = defaultdict(lambda: {'number_of_speakers': 0, 'number_of_files': 0, 'total_duration': 0, 'number_of_males': 0, 'number_of_females': 0})\n",
    "\n",
    "# Read data from the existing CSV file\n",
    "with open(input_csv_path, 'r') as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    for row in reader:\n",
    "        district = row['District']\n",
    "        state = row['State']\n",
    "        num_speakers = int(row['Number of Audio Files'])\n",
    "        duration = float(row['Total Duration (s)'])\n",
    "        gender = row['Gender']\n",
    "\n",
    "        # Aggregate data based on district and state\n",
    "        aggregated_data[(district, state)]['number_of_speakers'] += 1\n",
    "        aggregated_data[(district, state)]['number_of_files'] += num_speakers\n",
    "        aggregated_data[(district, state)]['total_duration'] += (duration/3600)\n",
    "        if gender.strip() == 'Male':\n",
    "            aggregated_data[(district, state)]['number_of_males'] += 1\n",
    "        elif gender.strip() == 'Female':\n",
    "            aggregated_data[(district, state)]['number_of_females'] += 1\n",
    "\n",
    "# Write aggregated data to the new CSV file\n",
    "with open(output_csv_path, 'w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['District', 'State', 'Number of Speakers', 'Number of Files', 'Total Duration (hr)', 'Number of Males', 'Number of Females'])\n",
    "    for (district, state), data in aggregated_data.items():\n",
    "        writer.writerow([district, state, data['number_of_speakers'], data['number_of_files'], data['total_duration'], data['number_of_males'], data['number_of_females']])\n",
    "\n",
    "print(\"Aggregated data written to CSV successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  made the csv which contains all the information State Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data written to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to the existing CSV file\n",
    "input_csv_path = '/home/vaibh/Vaani/Speaker_ID/CSVs/megdap/aggregated_data_district_wise_megdap.csv'\n",
    "\n",
    "# Path to the new CSV file to be created\n",
    "output_csv_path = 'aggregated_data_statewise.csv'\n",
    "\n",
    "# Dictionary to store aggregated data for each state\n",
    "aggregated_data_statewise = defaultdict(lambda: {'number_of_speakers': 0, 'number_of_files': 0, 'total_duration_hours': 0, 'number_of_males': 0, 'number_of_females': 0, 'district_count': 0})\n",
    "\n",
    "# Read data from the existing CSV file\n",
    "with open(input_csv_path, 'r') as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "    for row in reader:\n",
    "        district = row['District']\n",
    "        state = row['State']\n",
    "        num_speakers = int(row['Number of Speakers'])\n",
    "        num_files = int(row['Number of Files'])\n",
    "        total_duration_hours = float(row['Total Duration (hr)'])\n",
    "        num_males = int(row['Number of Males'])\n",
    "        num_females = int(row['Number of Females'])\n",
    "\n",
    "        # Aggregate data based on state\n",
    "        aggregated_data_statewise[state]['number_of_speakers'] += num_speakers\n",
    "        aggregated_data_statewise[state]['number_of_files'] += num_files\n",
    "        aggregated_data_statewise[state]['total_duration_hours'] += total_duration_hours\n",
    "        aggregated_data_statewise[state]['number_of_males'] += num_males\n",
    "        aggregated_data_statewise[state]['number_of_females'] += num_females\n",
    "\n",
    "        # Count the number of unique districts in each state\n",
    "        if district not in aggregated_data_statewise[state]:\n",
    "            aggregated_data_statewise[state]['district_count'] += 1\n",
    "\n",
    "# Write aggregated data to the new CSV file\n",
    "with open(output_csv_path, 'w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['State', 'Number of Speakers', 'Number of Files', 'Total Duration (hr)', 'Number of Males', 'Number of Females', 'District Count'])\n",
    "    for state, data in aggregated_data_statewise.items():\n",
    "        writer.writerow([state, data['number_of_speakers'], data['number_of_files'], data['total_duration_hours'], data['number_of_males'], data['number_of_females'], data['district_count']])\n",
    "\n",
    "print(\"Aggregated data written to CSV successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique speaker in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def get_unique_speaker_ids(folder_path):\n",
    "    # List all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #print(file_list[0])\n",
    "    # Set to store unique speaker IDs\n",
    "    unique_speaker_ids = set()\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for file_name in file_list:\n",
    "        # Extract speaker ID from the filename\n",
    "        if file_name.endswith('.wav'):\n",
    "            speaker_id = file_name.split('_')[5]\n",
    "            \n",
    "            # Add speaker ID to the set\n",
    "            unique_speaker_ids.add(speaker_id)\n",
    "    \n",
    "    return unique_speaker_ids\n",
    "\n",
    "def write_unique_speaker_ids_to_csv(unique_speaker_ids, output_csv_file):\n",
    "    with open(output_csv_file, 'w', newline='') as csvfile: \n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for speaker_id in unique_speaker_ids:\n",
    "            csv_writer.writerow([speaker_id])\n",
    "\n",
    "# Parent folder containing multiple subfolders with .wav files\n",
    "parent_folder = '/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2'\n",
    "\n",
    "# Output CSV file to store combined unique speaker IDs\n",
    "combined_csv_file = '/raid/scratch/Vaibhav/Dataset/combined_unique_speaker_ids.csv'\n",
    "\n",
    "# Initialize an empty set to store all unique speaker IDs\n",
    "all_unique_speaker_ids = set()\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for folder in os.listdir(parent_folder):\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(parent_folder, folder)\n",
    "\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "\n",
    "        print(subfolder)\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        #print(subfolder_path)\n",
    "        # Get unique speaker IDs for the current subfolder\n",
    "        unique_speaker_ids = get_unique_speaker_ids(subfolder_path)\n",
    "\n",
    "        # Add unique speaker IDs to the set of all unique speaker IDs\n",
    "        all_unique_speaker_ids.update(unique_speaker_ids)\n",
    "\n",
    "# Write all unique speaker IDs to the combined CSV file\n",
    "write_unique_speaker_ids_to_csv(all_unique_speaker_ids, combined_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  count of lanuages one speaker can speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 0/14128 [00:00<?, ?ID/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 1/14128 [00:01<5:53:27,  1.50s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MAITHILI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 2/14128 [00:03<6:51:03,  1.75s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 3/14128 [00:04<6:19:45,  1.61s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 4/14128 [00:06<6:06:49,  1.56s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 5/14128 [00:07<5:59:00,  1.53s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 6/14128 [00:09<5:54:52,  1.51s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MARATHI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 7/14128 [00:11<6:26:11,  1.64s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 8/14128 [00:12<6:12:36,  1.58s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RAJASTHANI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 9/14128 [00:14<6:37:52,  1.69s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HINDI'] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Speaker IDs:   0%|          | 9/14128 [00:16<6:59:08,  1.78s/ID]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker ID counts, number of folders, and corresponding folder names have been written to: /raid/scratch/Vaibhav/Dataset/speaker_id_counts_with_lanuages.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_speaker_id_from_filename(filename):\n",
    "    # Extract speaker ID from the filename\n",
    "    if filename.endswith('.wav'):\n",
    "        return filename.split('_')[5]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def count_folders_with_speaker_id(parent_folder, speaker_id):\n",
    "    count = 0\n",
    "    folders = []\n",
    "    \n",
    "    # Iterate over each subfolder\n",
    "    for folder in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder)\n",
    "\n",
    "        for subfolder in os.listdir(folder_path):\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            #print(subfolder)\n",
    "\n",
    "            # Check if the folder contains .wav files with the specified speaker ID\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                for file_name in os.listdir(subfolder_path):\n",
    "                    if get_speaker_id_from_filename(file_name) == speaker_id:\n",
    "                        folders.append(subfolder)\n",
    "                        count += 1\n",
    "                        print(folders, count)\n",
    "                        break  # No need to check other files in this folder\n",
    "        \n",
    "    return count, folders\n",
    "\n",
    "# Parent folder containing multiple subfolders with .wav files\n",
    "parent_folder = '/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2'\n",
    "\n",
    "# CSV file containing all unique speaker IDs\n",
    "unique_speaker_ids_csv = '/raid/scratch/Vaibhav/Dataset/combined_unique_speaker_ids.csv'\n",
    "\n",
    "# Output CSV file to store speaker IDs, their counts, and corresponding folder names\n",
    "output_csv_file = '/raid/scratch/Vaibhav/Dataset/speaker_id_counts_with_lanuages.csv'\n",
    "\n",
    "# Read the CSV file containing all unique speaker IDs\n",
    "all_unique_speaker_ids = []\n",
    "with open(unique_speaker_ids_csv, 'r') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        all_unique_speaker_ids.append(row[0])\n",
    "\n",
    "# Create a list to store speaker ID counts, number of folders, and corresponding folder names\n",
    "speaker_id_counts_with_folders = []\n",
    "\n",
    "# Create a tqdm progress bar for all unique speaker IDs\n",
    "pbar = tqdm(total=len(all_unique_speaker_ids), desc=\"Processing Speaker IDs\", unit=\"ID\")\n",
    "\n",
    "count_speaker = 0\n",
    "\n",
    "# Count the number of folders containing each speaker ID and get the folder names\n",
    "for speaker_id in all_unique_speaker_ids:\n",
    "    count_speaker += 1\n",
    "\n",
    "    count, folders = count_folders_with_speaker_id(parent_folder, speaker_id)\n",
    "    speaker_id_counts_with_folders.append([speaker_id, count, ', '.join(folders)])\n",
    "\n",
    "    if count_speaker >= 10:\n",
    "        break\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Write speaker ID counts, number of folders, and folder names to the output CSV file\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Speaker ID', 'Number of Folders', 'Folders'])\n",
    "    csv_writer.writerows(speaker_id_counts_with_folders)\n",
    "\n",
    "print(\"Speaker ID counts, number of folders, and corresponding folder names have been written to:\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure out in how many cases same speaker's file get detected with different languages and to what extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data has been written to speakerid_true_pred_different1.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Input folder containing CSV files\n",
    "input_folder = \"/raid/scratch/Vaibhav/csv_predicted_facebook_equal(S,M)_part2/\"\n",
    "\n",
    "# Output CSV file\n",
    "output_csv_file = \"speakerid_true_pred_different1.csv\"\n",
    "\n",
    "# Dictionary to store aggregated data for each speaker ID\n",
    "speaker_data = {}\n",
    "\n",
    "# Iterate over each CSV file in the input folder\n",
    "for folder in os.listdir(input_folder):\n",
    "    folder_path = os.path.join(input_folder, folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_csv_file = os.path.join(folder_path, filename)\n",
    "\n",
    "            with open(input_csv_file, mode='r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    speaker_id = os.path.basename(row['Filename']).split('_')[5]\n",
    "                    district = os.path.basename(row['Filename']).split('_')[4]\n",
    "                    state = os.path.basename(row['Filename']).split('_')[3]\n",
    "\n",
    "                    if speaker_id not in speaker_data:\n",
    "                        speaker_data[speaker_id] = {\n",
    "                            'Vendors': set(),\n",
    "                            'Languages': {\n",
    "                                'Asserted': {},\n",
    "                                'Predicted': {}\n",
    "                            },\n",
    "                            'District': district,\n",
    "                            'State': state\n",
    "                        }\n",
    "\n",
    "                    speaker_data[speaker_id]['Vendors'].add(row['Vendor'])\n",
    "\n",
    "                    # Count asserted languages\n",
    "                    asserted_language = row['Asserted Language']\n",
    "                    if asserted_language not in speaker_data[speaker_id]['Languages']['Asserted']:\n",
    "                        speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] = 0\n",
    "                    speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] += 1\n",
    "\n",
    "                    # Count predicted languages\n",
    "                    predicted_language = row['Detected Language']\n",
    "                    if predicted_language not in speaker_data[speaker_id]['Languages']['Predicted']:\n",
    "                        speaker_data[speaker_id]['Languages']['Predicted'][predicted_language] = 0\n",
    "                    speaker_data[speaker_id]['Languages']['Predicted'][predicted_language] += 1\n",
    "\n",
    "# Write aggregated data to output CSV file\n",
    "with open(output_csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['SpeakerID', 'Vendors', 'Asserted_Languages', 'Asserted_Languages_Count', 'Predicted_Languages', 'Predicted_Languages_Count', 'District', 'State'])\n",
    "    for speaker_id, data in speaker_data.items():\n",
    "        vendors = ', '.join(data['Vendors'])\n",
    "        asserted_languages = ', '.join(data['Languages']['Asserted'].keys())\n",
    "        asserted_languages_count = ', '.join(str(count) for count in data['Languages']['Asserted'].values())\n",
    "        predicted_languages = ', '.join(data['Languages']['Predicted'].keys())\n",
    "        predicted_languages_count = ', '.join(str(count) for count in data['Languages']['Predicted'].values())\n",
    "        writer.writerow([speaker_id, vendors, asserted_languages, asserted_languages_count, predicted_languages, predicted_languages_count, data['District'], data['State']])\n",
    "\n",
    "print(\"Aggregated data has been written to\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'SpeakerID': '61551', 'Vendors': 'S', 'Asserted_Languages': 'Khari boli', 'Predicted_Languages': 'Hindi', 'District': 'JyotibaPhuleNagar', 'State': 'Uttarpradesh'}\n",
      "4 {'SpeakerID': '61701', 'Vendors': 'S', 'Asserted_Languages': 'Khari boli', 'Predicted_Languages': 'Hindi, Urdu, Gujarati, Sindhi', 'District': 'JyotibaPhuleNagar', 'State': 'Uttarpradesh'}\n",
      "5 {'SpeakerID': '142462', 'Vendors': 'S', 'Asserted_Languages': 'Khortha', 'Predicted_Languages': 'Hindi, Marathi, Sindhi, Urdu, Nepali (individual language)', 'District': 'Madhepura', 'State': 'Bihar'}\n",
      "7 {'SpeakerID': '152995', 'Vendors': 'S', 'Asserted_Languages': 'Malvani', 'Predicted_Languages': 'Hindi, Marathi, Sindhi, Sinhala, Pushto, Urdu, Telugu', 'District': 'Solapur', 'State': 'Maharashtra'}\n",
      "10 {'SpeakerID': '61195', 'Vendors': 'S', 'Asserted_Languages': 'Jaipuri', 'Predicted_Languages': 'Southern Balochi, Assamese, Hindi, Kannada, Gujarati, Kashmiri, Pushto, Urdu, Panjabi, Telugu', 'District': 'Churu', 'State': 'Rajasthan'}\n",
      "13 {'SpeakerID': '106901', 'Vendors': 'S', 'Asserted_Languages': 'Nepalese', 'Predicted_Languages': 'Assamese, Hindi, Bengali, Marathi, Gujarati, Mahasu Pahari, Sindhi, Haitian, Portuguese, Telugu, Panjabi, Nepali (individual language), Bhojpuri', 'District': 'Vaishali', 'State': 'Bihar'}\n",
      "20 {'SpeakerID': 'Sahi59061', 'Vendors': 'M', 'Asserted_Languages': 'Bundeli', 'Predicted_Languages': 'Bengali, Bundeli, Hinduri, Chhattisgarhi, Khasi, Sanskrit, Panjabi, Wagdi, Western Panjabi, Southern Balochi, Hindi, Gujarati, Pahari-Potwari, Saraiki, Sinhala, Marwari (Pakistan), Urdu, Garhwali, Mewati, Sindhi', 'District': 'Hamirpur', 'State': 'UP'}\n",
      "Number of occurrences with 1 predicted labels for a speaker: 1182\n",
      "Number of occurrences with 2 predicted labels for a speaker: 213\n",
      "Number of occurrences with 3 predicted labels for a speaker: 54\n",
      "Number of occurrences with 4 predicted labels for a speaker: 19\n",
      "Number of occurrences with 5 predicted labels for a speaker: 13\n",
      "Number of occurrences with 6 predicted labels for a speaker: 3\n",
      "Number of occurrences with 7 predicted labels for a speaker: 6\n",
      "Number of occurrences with 8 predicted labels for a speaker: 2\n",
      "Number of occurrences with 10 predicted labels for a speaker: 1\n",
      "Number of occurrences with 11 predicted labels for a speaker: 1\n",
      "Number of occurrences with 13 predicted labels for a speaker: 1\n",
      "Number of occurrences with 20 predicted labels for a speaker: 1\n",
      "Maximum no. of predicted labels: 20\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Input CSV file\n",
    "input_csv_file = \"/raid/scratch/Vaibhav/speakerid_true_pred_different.csv\"\n",
    "\n",
    "# Dictionary to store counts for different lengths of predicted labels\n",
    "label_counts = {i: 0 for i in range(1, 21)}\n",
    "\n",
    "# Variable to store the maximum length of predicted labels\n",
    "max_length = 0\n",
    "\n",
    "# Read data from input CSV file\n",
    "with open(input_csv_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        predicted_languages = row['Predicted_Languages'].split(', ')\n",
    "        num_predicted_languages = len(predicted_languages)\n",
    "        \n",
    "        # Increment count based on the length of predicted labels\n",
    "        if num_predicted_languages in label_counts:\n",
    "            label_counts[num_predicted_languages] += 1\n",
    "        \n",
    "        # Update the maximum length\n",
    "        if num_predicted_languages > max_length:\n",
    "            ans_row = row\n",
    "            print(num_predicted_languages, row)\n",
    "            max_length = num_predicted_languages\n",
    "\n",
    "# Print the counts for different lengths of predicted labels\n",
    "for length, count in label_counts.items():\n",
    "    if count != 0 :\n",
    "        print(f\"Number of occurrences with {length} predicted labels for a speaker: {count}\")\n",
    "\n",
    "#print(ans_row)\n",
    "# Print the maximum length of predicted labels\n",
    "print(f\"Maximum no. of predicted labels: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaibhav_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
