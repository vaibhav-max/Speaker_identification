{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import csv\n",
    "\n",
    "def get_language_name(iso_code):\n",
    "    try:\n",
    "        language = pycountry.languages.get(alpha_3=iso_code)\n",
    "        return language.name\n",
    "    except AttributeError:\n",
    "        return \"Language not found\"\n",
    "\n",
    "# Read ISO codes from the text file\n",
    "file_path = \"C:\\Users\\Vaibhav\\Desktop\\Vaani\\iso_codes.txt\"  # Replace 'iso_codes.txt' with the path to your text file\n",
    "with open(file_path, 'r') as file:\n",
    "    iso_codes = file.readlines()\n",
    "\n",
    "# Remove any leading/trailing whitespace and newline characters\n",
    "iso_codes = [code.strip() for code in iso_codes]\n",
    "\n",
    "# Write ISO codes and language names to a CSV file\n",
    "with open(\"language_names.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"ISO Code\", \"Language Name\"])  # Write headers\n",
    "    for code in iso_codes:\n",
    "        language_name = get_language_name(code)\n",
    "        writer.writerow([code, language_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  audio is mono or stereo and sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "def is_stereo_or_mono(audio_file):\n",
    "    y, sr = librosa.load(audio_file, mono=False, sr =None)\n",
    "    print(sr)\n",
    "    if y.ndim == 1:\n",
    "        return \"Mono\"\n",
    "    elif y.ndim == 2:\n",
    "        return \"Stereo\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Path to the folder containing audio files\n",
    "folder_path = \"/raid/scratch/Vaibhav/Dataset/Audios/Hamirpur\"\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".mp3\") or filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        result = is_stereo_or_mono(file_path)\n",
    "        print(f\"{filename}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select 150 files from each lanuage such that vender S and M are in Equal proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected .wav files copied successfully to the destination folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def select_files(folder_path):\n",
    "    # Initialize counters for each category\n",
    "    count_M = 0\n",
    "    count_S = 0\n",
    "\n",
    "    # List to store file paths\n",
    "    file_paths_M = []\n",
    "    file_paths_S = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".wav\"):\n",
    "            if filename.split(\"_\")[2] == \"M\":\n",
    "                file_paths_M.append(file_path)\n",
    "                count_M += 1\n",
    "            elif filename.split(\"_\")[2] == \"S\":\n",
    "                file_paths_S.append(file_path)\n",
    "                count_S += 1\n",
    "        \n",
    "            # Break the loop if 150 files are selected or one of the categories is exhausted\n",
    "            if count_M >= 75 and count_S >= 75:\n",
    "                break\n",
    "    \n",
    "    if count_M == 0 or count_S == 0:\n",
    "        num_files_to_select = 75\n",
    "    else:\n",
    "        num_files_to_select = min(75, min(count_M, count_S))\n",
    "\n",
    "    # Shuffle the file paths to randomize the order\n",
    "    random.shuffle(file_paths_M)\n",
    "    random.shuffle(file_paths_S)\n",
    "\n",
    "    # Select files from each category in equal proportion\n",
    "    selected_files = file_paths_M[:num_files_to_select] + file_paths_S[:num_files_to_select]\n",
    "\n",
    "    # Shuffle the selected files to randomize the order\n",
    "    random.shuffle(selected_files)\n",
    "\n",
    "    return selected_files\n",
    "\n",
    "# Path to the parent directory containing all folders\n",
    "parent_directory = \"/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2/Not_Supported_facebook\"\n",
    "\n",
    "# Destination folder where selected files will be copied\n",
    "destination_parent_folder = \"/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2_150Each/Not_Supported_facebook\"\n",
    "\n",
    "# Create the destination parent folder if it does not exist\n",
    "if not os.path.exists(destination_parent_folder):\n",
    "    os.makedirs(destination_parent_folder)\n",
    "\n",
    "# Iterate over each folder in the parent directory\n",
    "for folder_name in os.listdir(parent_directory):\n",
    "    folder_path = os.path.join(parent_directory, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        selected_files = select_files(folder_path)\n",
    "        \n",
    "        # Create a folder with the same name in the destination parent folder\n",
    "        destination_folder = os.path.join(destination_parent_folder, folder_name)\n",
    "        if not os.path.exists(destination_folder):\n",
    "            os.makedirs(destination_folder)\n",
    "        \n",
    "        # Copy selected files into the destination folder\n",
    "        for file_path in selected_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            destination_path = os.path.join(destination_folder, filename)\n",
    "            shutil.copy(file_path, destination_path)\n",
    "\n",
    "print(\"Selected .wav files copied successfully to the destination folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 75\n",
      "75 75\n",
      "75 0\n",
      "0 75\n",
      "0 46\n",
      "0 75\n",
      "0 56\n",
      "0 75\n",
      "0 75\n",
      "75 0\n",
      "75 75\n",
      "75 75\n"
     ]
    }
   ],
   "source": [
    "def select_files(folder_path):\n",
    "    # Initialize counters for each category\n",
    "    count_M = 0\n",
    "    count_S = 0\n",
    "\n",
    "    # List to store file paths\n",
    "    file_paths_M = []\n",
    "    file_paths_S = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".wav\"):\n",
    "            if filename.split(\"_\")[2] == \"M\":\n",
    "                file_paths_M.append(file_path)\n",
    "                count_M += 1\n",
    "            elif filename.split(\"_\")[2] == \"S\":\n",
    "                file_paths_S.append(file_path)\n",
    "                count_S += 1\n",
    "    \n",
    "    print(count_M, count_S)\n",
    "\n",
    "# Destination folder where selected files will be copied\n",
    "destination_parent_folder = \"/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2_150Each/Not_Supported_facebook\"\n",
    "\n",
    "# Iterate over each folder in the parent directory\n",
    "for folder_name in os.listdir(destination_parent_folder):\n",
    "    folder_path = os.path.join(destination_parent_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        select_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change the folder name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the parent directory containing folders\n",
    "parent_directory = \"/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2/Not_Supported_facebook\"\n",
    "\n",
    "# Iterate over each folder in the parent directory\n",
    "for folder_name in os.listdir(parent_directory):\n",
    "    folder_path = os.path.join(parent_directory, folder_name)\n",
    "    if os.path.isdir(folder_path) and folder_name.endswith(\"_not_supported\"):\n",
    "        # Remove the \"_not_supported\" suffix from the folder name\n",
    "        new_folder_name = folder_name.split('_')[0]  # Assuming \"_not_supported\" has 14 characters\n",
    "        new_folder_path = os.path.join(parent_directory, new_folder_name)\n",
    "        os.rename(folder_path, new_folder_path)\n",
    "\n",
    "print(\"Folders renamed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change the names of folders in a directory such that only the first letter is capitalized and the rest are in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def capitalize_folder_names(directory):\n",
    "    # Iterate over each item in the directory\n",
    "    for item in os.listdir(directory):\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            # Capitalize the first letter and convert the rest to lowercase\n",
    "            new_name = item.capitalize()\n",
    "            # Rename the folder\n",
    "            os.rename(os.path.join(directory, item), os.path.join(directory, new_name))\n",
    "\n",
    "# Specify the directory containing the folders\n",
    "directory_path = '/home/vaibh/Vaani/Dataset/Audio_language_specific_part2_150Each/Not_Supported_facebook'\n",
    "\n",
    "# Call the function to capitalize folder names\n",
    "capitalize_folder_names(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve the info from json file make one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with unique speaker IDs and associated information has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to extract speaker ID from audio filename\n",
    "def extract_speaker_id(audio_filename):\n",
    "    return audio_filename.split('/')[-1].split('_')[5]\n",
    "\n",
    "# Load JSON data\n",
    "with open('/home/vaibh/Vaani/Json/dataWmT.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create a dictionary to store unique speaker IDs and their associated information\n",
    "speaker_info_map = {}\n",
    "\n",
    "# Extract required information from JSON\n",
    "for entry in data:\n",
    "    speaker_id = extract_speaker_id(entry['audioFilename'])\n",
    "    if speaker_id not in speaker_info_map:\n",
    "        speaker_info_map[speaker_id] = {\n",
    "            'state': entry['state'],\n",
    "            'district': entry['district'],\n",
    "            'gender': entry['gender'],\n",
    "            'pincode': entry['pincode'],\n",
    "            'assertLanguage': entry['assertLanguage']\n",
    "        }\n",
    "\n",
    "# Write speaker IDs and their associated information to a new CSV file\n",
    "with open('speaker_info.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['Speaker ID', 'State', 'District', 'Gender', 'Pincode', 'Assert Language'])\n",
    "    for speaker_id, info in speaker_info_map.items():\n",
    "        writer.writerow([speaker_id, info['state'], info['district'], info['gender'], info['pincode'], info['assertLanguage']])\n",
    "\n",
    "print(\"CSV file with unique speaker IDs and associated information has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add the lanuage info to shaip and megdap csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged CSV file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'Speaker ID' and 'Assert Language' columns from the first CSV file\n",
    "df_speaker_language = pd.read_csv('/home/vaibh/Vaani/Json/speaker_info_total.csv', usecols=['Speaker ID', 'Assert Language'])\n",
    "\n",
    "# Load the second CSV file\n",
    "df_additional_info = pd.read_csv('/home/vaibh/Vaani/Speaker_ID/CSVs/megdap/speaker_info_megdap.csv')\n",
    "\n",
    "# Convert 'Speaker ID' column in df_additional_info to match the data type in df_speaker_language\n",
    "df_additional_info['Speaker ID'] = df_additional_info['Speaker ID'].astype(str)\n",
    "\n",
    "# Merge the DataFrames based on the 'Speaker ID' column\n",
    "df_final = pd.merge(df_additional_info, df_speaker_language, on='Speaker ID', how='left')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "df_final.to_csv('final_merged_speaker_info.csv', index=False)\n",
    "\n",
    "print(\"Final merged CSV file has been created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# divide the data into train test val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train, test, and val sets with balanced genders, districts, and languages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('/home/vaibh/Vaani/Speaker_ID/CSVs/shaip/speaker_info_lan_shaip.csv')\n",
    "\n",
    "# Define the columns for stratified sampling\n",
    "stratify_columns = ['District']\n",
    "\n",
    "# Split the data into train and temp sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, stratify=df[stratify_columns], random_state=42)\n",
    "\n",
    "# Exclude the problematic districts from the temporary set and merge the remaining temp set with the training set\n",
    "temp_df_excluded = temp_df[~temp_df['District'].isin(train_df['District'])]\n",
    "train_df = pd.concat([train_df, temp_df_excluded], axis=0)\n",
    "\n",
    "# Filter out districts with fewer than 2 instances in the remaining data\n",
    "temp_df_filtered = temp_df[temp_df['District'].isin(train_df['District'])]\n",
    "district_counts = temp_df_filtered['District'].value_counts()\n",
    "valid_districts = district_counts[district_counts >= 2].index\n",
    "temp_df_filtered = temp_df_filtered[temp_df_filtered['District'].isin(valid_districts)]\n",
    "\n",
    "# Use the filtered data for test and validation sets\n",
    "test_df, val_df = train_test_split(temp_df_filtered, test_size=0.5, stratify=temp_df_filtered[stratify_columns], random_state=42)\n",
    "\n",
    "# Save the split datasets to separate CSV files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "\n",
    "print(\"Data split into train, test, and val sets with balanced genders, districts, and languages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report has been saved to dataset_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the split datasets\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "\n",
    "# Function to get counts for a specific column\n",
    "def get_counts(df, column_name):\n",
    "    return df[column_name].value_counts()\n",
    "\n",
    "# Get counts for districts, states, genders, and assert languages in train dataset\n",
    "train_district_counts = get_counts(train_df, 'District')\n",
    "train_state_counts = get_counts(train_df, 'State')\n",
    "train_gender_counts = get_counts(train_df, 'Gender')\n",
    "train_assert_language_counts = get_counts(train_df, 'Assert Language')\n",
    "\n",
    "# Get counts for districts, states, genders, and assert languages in test dataset\n",
    "test_district_counts = get_counts(test_df, 'District')\n",
    "test_state_counts = get_counts(test_df, 'State')\n",
    "test_gender_counts = get_counts(test_df, 'Gender')\n",
    "test_assert_language_counts = get_counts(test_df, 'Assert Language')\n",
    "\n",
    "# Get counts for districts, states, genders, and assert languages in validation dataset\n",
    "val_district_counts = get_counts(val_df, 'District')\n",
    "val_state_counts = get_counts(val_df, 'State')\n",
    "val_gender_counts = get_counts(val_df, 'Gender')\n",
    "val_assert_language_counts = get_counts(val_df, 'Assert Language')\n",
    "\n",
    "# Define the file name for the report\n",
    "report_file = 'dataset_report.txt'\n",
    "\n",
    "# Open the file in write mode and write the report\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"Train Dataset:\\n\")\n",
    "    f.write(\"District Counts:\\n\" + str(train_district_counts) + \"\\n\")\n",
    "    f.write(\"State Counts:\\n\" + str(train_state_counts) + \"\\n\")\n",
    "    f.write(\"Gender Counts:\\n\" + str(train_gender_counts) + \"\\n\")\n",
    "    f.write(\"Assert Language Counts:\\n\" + str(train_assert_language_counts) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Test Dataset:\\n\")\n",
    "    f.write(\"District Counts:\\n\" + str(test_district_counts) + \"\\n\")\n",
    "    f.write(\"State Counts:\\n\" + str(test_state_counts) + \"\\n\")\n",
    "    f.write(\"Gender Counts:\\n\" + str(test_gender_counts) + \"\\n\")\n",
    "    f.write(\"Assert Language Counts:\\n\" + str(test_assert_language_counts) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Validation Dataset:\\n\")\n",
    "    f.write(\"District Counts:\\n\" + str(val_district_counts) + \"\\n\")\n",
    "    f.write(\"State Counts:\\n\" + str(val_state_counts) + \"\\n\")\n",
    "    f.write(\"Gender Counts:\\n\" + str(val_gender_counts) + \"\\n\")\n",
    "    f.write(\"Assert Language Counts:\\n\" + str(val_assert_language_counts) + \"\\n\")\n",
    "\n",
    "print(\"Report has been saved to\", report_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in csv change the audio filename to web link  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/AWADHI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/MAITHILI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/MARATHI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/SURJAPURI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/HINDI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/ENGLISH_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/HALBI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/SHEKHAWATI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/ODIA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/DURUWA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/AGARIYA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BENGALI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/LAMBADI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BUNDELI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BHILI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/MAGAHI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/GARHWALI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/SURGUJIA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BHOJPURI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/MEWARI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/ANGIKA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/TULU_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/KANNADA_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/SADRI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/MALAYALAM_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/TELUGU_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/KUMAONI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BAGRI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/BHATRI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/TAMIL_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/ASSAMESE_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/SANTALI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/WAGDI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/GUJARATI_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/URDU_predicted_labels.csv\n",
      "Processed CSV: /data/Vaani/CSVs/Supported_facebook/KURUKH_predicted_labels.csv\n",
      "All CSV files processed!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def convert_data_to_link(data):\n",
    "  # Base URL for the web link\n",
    "  base_url = \"https://vaani.iisc.ac.in/Audios/\"\n",
    "\n",
    "  # New list to store data with web link and vendor name\n",
    "  data_with_link_and_vendor = []\n",
    "  for row in data:\n",
    "    filename = row[0]\n",
    "    # Extract vendor name and district name from filename\n",
    "    vendor = filename.split('_')[2]\n",
    "    district_name = filename.split('_')[4]\n",
    "    # Generate web link\n",
    "    web_link = f\"{base_url}{district_name}/{filename}\"\n",
    "\n",
    "    # Create a new row with web link and vendor name\n",
    "    new_row = [web_link, vendor] + row[1:]  # Add web link, vendor, and remaining columns\n",
    "    data_with_link_and_vendor.append(new_row)\n",
    "  return data_with_link_and_vendor\n",
    "\n",
    "# Function to read data from CSV file\n",
    "def read_data_from_csv(filename):\n",
    "  data = []\n",
    "  with open(filename, 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip header row\n",
    "    for row in reader:\n",
    "      data.append(row)\n",
    "  return data\n",
    "\n",
    "# Define directory containing CSV files (replace with your actual directory)\n",
    "csv_directory = \"/data/Vaani/CSVs/Supported_facebook\"\n",
    "\n",
    "# Output directory for processed CSV files (optional, create if needed)\n",
    "output_directory = \"/data/Root_content/Vaani/CSVs/Supported_facebook\"  # Uncomment if desired\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "  if filename.endswith(\".csv\"):  # Check for CSV files only\n",
    "    input_filename = os.path.join(csv_directory, filename)\n",
    "    output_filename = os.path.join(output_directory, f\"{filename}\")  # Optional for separate output directory\n",
    "\n",
    "    # Read data from CSV file\n",
    "    data = read_data_from_csv(input_filename)\n",
    "\n",
    "    # Convert data to include web links and vendor names\n",
    "    data_with_link_and_vendor = convert_data_to_link(data)\n",
    "\n",
    "    # Open file for writing in CSV format\n",
    "    with open(output_filename, 'w', newline='') as csvfile:\n",
    "      writer = csv.writer(csvfile)\n",
    "      writer.writerow([\"File\", \"Vendor\", \"Asserted_Language\", \"Detected_Language\", \"Probability\"])  # Write header\n",
    "      writer.writerows(data_with_link_and_vendor)\n",
    "\n",
    "    print(f\"Processed CSV: {input_filename}\")\n",
    "\n",
    "print(\"All CSV files processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding supported and not supported in the summary csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160984/2265550753.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/data/Vaani/speakerid_true_pred_different_whole_data.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('/data/Vaani/speakerid_true_pred_different_whole_data.csv')\n",
    "\n",
    "# Read TXT file containing not supported languages\n",
    "with open('/data/Vaani/Not_supported_language_facebook_model.txt', 'r') as f:\n",
    "    not_supported_languages = set(line.strip().lower() for line in f)\n",
    "\n",
    "# Function to check if a language is supported or not\n",
    "def check_support(language):\n",
    "    if language.strip().lower() in not_supported_languages:\n",
    "        return 'not_supported'\n",
    "    else:\n",
    "        return 'supported'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['Supported_Status'] = df['Asserted_Language'].apply(check_support)\n",
    "\n",
    "# Insert the new column after the Asserted_Language column\n",
    "column_index = df.columns.get_loc('Asserted_Language') + 1\n",
    "df.insert(column_index, 'Supported_Status', df.pop('Supported_Status'))\n",
    "\n",
    "# Write the updated DataFrame back to CSV\n",
    "df.to_csv('updated_csv_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making the csv file which contains the info assert lanuage wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define a function to parse the data and aggregate it by asserted language\n",
    "def aggregate_by_asserted_language(input_file):\n",
    "    language_aggregated_data = {}\n",
    "\n",
    "    with open(input_file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            asserted_languages = row['Asserted_Language'].split(', ')\n",
    "            for asserted_language in asserted_languages:\n",
    "                if asserted_language not in language_aggregated_data:\n",
    "                    language_aggregated_data[asserted_language] = {\n",
    "                        'SpeakerID': [],\n",
    "                        'Vendors': set(),  # Change to set\n",
    "                        'Supported_Status': [],\n",
    "                        'Files_count': [],\n",
    "                        'Predicted_Languages': [],\n",
    "                        'Predicted_Language_Counts': [],\n",
    "                        'Predicted_Language_Probabilities_mean': [],\n",
    "                        'District': set(),  # Change to set\n",
    "                        'State': set()     # Change to set\n",
    "                    }\n",
    "                language_aggregated_data[asserted_language]['SpeakerID'].append(row['SpeakerID'])\n",
    "                language_aggregated_data[asserted_language]['Vendors'].add(row['Vendors'])  # Add to set\n",
    "                language_aggregated_data[asserted_language]['Supported_Status'].append(row['Supported_Status'])\n",
    "                language_aggregated_data[asserted_language]['Files_count'].append(row['Files_count'])\n",
    "                language_aggregated_data[asserted_language]['Predicted_Languages'].append(row['Predicted_Languages'])\n",
    "                language_aggregated_data[asserted_language]['Predicted_Language_Counts'].append(row['Predicted_Language_Counts'])\n",
    "                language_aggregated_data[asserted_language]['Predicted_Language_Probabilities_mean'].append(row['Predicted_Language_Probabilities_mean'])\n",
    "                language_aggregated_data[asserted_language]['District'].add(row['District'])  # Add to set\n",
    "                language_aggregated_data[asserted_language]['State'].add(row['State'])        # Add to set\n",
    "\n",
    "    return language_aggregated_data\n",
    "\n",
    "# Define a function to write the aggregated data to a new CSV file\n",
    "def write_aggregated_data_to_csv(aggregated_data, output_file):\n",
    "    with open(output_file, mode='w', newline='') as csvfile:\n",
    "        fieldnames = ['Asserted_Language', 'SpeakerID', 'Vendors', 'Supported_Status', 'Files_count', 'Predicted_Languages', 'Predicted_Language_Counts', 'Predicted_Language_Probabilities_mean', 'District', 'State']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for asserted_language, data in aggregated_data.items():\n",
    "            writer.writerow({\n",
    "                'Asserted_Language': asserted_language,\n",
    "                'SpeakerID': ', '.join(data['SpeakerID']),\n",
    "                'Vendors': ', '.join(data['Vendors']),  # Join set elements\n",
    "                'Supported_Status': ', '.join(data['Supported_Status']),\n",
    "                'Files_count': ', '.join(data['Files_count']),\n",
    "                'Predicted_Languages': ', '.join(data['Predicted_Languages']),\n",
    "                'Predicted_Language_Counts': ', '.join(data['Predicted_Language_Counts']),\n",
    "                'Predicted_Language_Probabilities_mean': ', '.join(data['Predicted_Language_Probabilities_mean']),\n",
    "                'District': ', '.join(data['District']),  # Join set elements\n",
    "                'State': ', '.join(data['State'])         # Join set elements\n",
    "            })\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    input_file = '/data/Vaani/speakerid_true_pred_summary_whole_data_support.csv'\n",
    "    output_file = 'aggregated_data_by_language.csv'\n",
    "    aggregated_data = aggregate_by_asserted_language(input_file)\n",
    "    write_aggregated_data_to_csv(aggregated_data, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/timit-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/timit-train loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "ds = deeplake.load(\"hub://activeloop/timit-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(ds))\n",
    "help(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://activeloop/timit-train', read_only=True, tensors=['audios', 'dialects', 'is_phoenetics', 'is_sentences', 'is_word_files', 'speaker_ids', 'texts'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaibhav_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
